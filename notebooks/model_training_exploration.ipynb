{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toyota GR Cup: Tire Degradation Model Training\n",
    "\n",
    "**Objective**: Predict tire degradation rate based on driving aggression metrics\n",
    "\n",
    "**Dataset**: \n",
    "- 2,545 laps from Toyota GR Cup racing series\n",
    "- 21 normalized features (brake pressure, G-forces, steering, throttle, speed, RPM)\n",
    "- Target: `tire_degradation_rate` (seconds per lap)\n",
    "\n",
    "**Models to Compare**:\n",
    "1. Linear Regression (baseline)\n",
    "2. Ridge Regression (L2 regularization)\n",
    "3. Lasso Regression (L1 regularization)\n",
    "4. Random Forest\n",
    "5. Gradient Boosting\n",
    "6. XGBoost (GPU-ready)\n",
    "\n",
    "**Evaluation Metrics**: RÂ², MAE, RMSE, Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"XGBoost GPU support: {xgb.build_info()['USE_CUDA']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and target\n",
    "features_path = Path('../ml_data/features_normalized.csv')\n",
    "target_path = Path('../ml_data/target_degradation.csv')\n",
    "\n",
    "X = pd.read_csv(features_path)\n",
    "y = pd.read_csv(target_path)['tire_degradation_rate']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Tire Degradation Rate (seconds/lap)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Variable Distribution')\n",
    "axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y, vert=True)\n",
    "axes[1].set_ylabel('Tire Degradation Rate (seconds/lap)')\n",
    "axes[1].set_title('Target Variable Box Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target variable range: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "print(f\"Outliers (> Q3 + 1.5*IQR): {sum(y > y.quantile(0.75) + 1.5 * (y.quantile(0.75) - y.quantile(0.25)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features (>0.9)\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr.append((correlation_matrix.columns[i], \n",
    "                            correlation_matrix.columns[j], \n",
    "                            correlation_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"\\nHighly correlated features (|r| > 0.9):\")\n",
    "    for feat1, feat2, corr in high_corr:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo highly correlated features found (threshold: |r| > 0.9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vs Target correlation\n",
    "feature_target_corr = X.corrwith(y).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_target_corr.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Correlation with Tire Degradation Rate')\n",
    "plt.title('Feature Importance: Correlation with Target Variable', fontsize=14, pad=20)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 positively correlated features:\")\n",
    "print(feature_target_corr.head())\n",
    "print(\"\\nTop 5 negatively correlated features:\")\n",
    "print(feature_target_corr.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train/Test ratio: {X_train.shape[0]/X_test.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "We'll train 6 different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=0.01, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        tree_method='auto',  # Change to 'gpu_hist' on Windows with CUDA\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Models configured. Ready for training.\")\n",
    "print(f\"\\nNote: XGBoost tree_method set to 'auto' (CPU).\")\n",
    "print(\"On Windows with GPU, change to 'gpu_hist' for CUDA acceleration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and collect results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_ev = explained_variance_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation (5-fold)\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, cv=5, \n",
    "        scoring='r2', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_ev': test_ev,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'train_time': train_time,\n",
    "        'predictions': y_test_pred\n",
    "    }\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Train RÂ²:     {train_r2:.4f}\")\n",
    "    print(f\"Test RÂ²:      {test_r2:.4f}\")\n",
    "    print(f\"Test MAE:     {test_mae:.4f}\")\n",
    "    print(f\"Test RMSE:    {test_rmse:.4f}\")\n",
    "    print(f\"Explained Var: {test_ev:.4f}\")\n",
    "    print(f\"CV RÂ² (5-fold): {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    print(f\"Training Time: {train_time:.2f}s\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_r2 - test_r2 > 0.1:\n",
    "        print(\"âš ï¸  Warning: Potential overfitting detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df[['test_r2', 'test_mae', 'test_rmse', 'cv_mean', 'cv_std', 'train_time']]\n",
    "comparison_df = comparison_df.sort_values('test_r2', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string())\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "best_model_name = comparison_df.index[0]\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test RÂ²: {comparison_df.loc[best_model_name, 'test_r2']:.4f}\")\n",
    "print(f\"   Test MAE: {comparison_df.loc[best_model_name, 'test_mae']:.4f}\")\n",
    "print(f\"   Test RMSE: {comparison_df.loc[best_model_name, 'test_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# RÂ² Score\n",
    "comparison_df['test_r2'].plot(kind='barh', ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_xlabel('RÂ² Score')\n",
    "axes[0, 0].set_title('Test RÂ² Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# MAE\n",
    "comparison_df['test_mae'].plot(kind='barh', ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_xlabel('Mean Absolute Error')\n",
    "axes[0, 1].set_title('Test MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# RMSE\n",
    "comparison_df['test_rmse'].plot(kind='barh', ax=axes[1, 0], color='lightgreen')\n",
    "axes[1, 0].set_xlabel('Root Mean Squared Error')\n",
    "axes[1, 0].set_title('Test RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Training Time\n",
    "comparison_df['train_time'].plot(kind='barh', ax=axes[1, 1], color='plum')\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 1].set_title('Training Time Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictions vs Actual (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(y_test, best_predictions, alpha=0.6, edgecolor='black', s=50)\n",
    "axes[0].plot([y_test.min(), y_test.max()], \n",
    "             [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Tire Degradation Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Tire Degradation Rate', fontsize=12)\n",
    "axes[0].set_title(f'{best_model_name}: Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - best_predictions\n",
    "axes[1].scatter(best_predictions, residuals, alpha=0.6, edgecolor='black', s=50)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Tire Degradation Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(f\"\\nResidual Statistics ({best_model_name}):\")\n",
    "print(f\"Mean: {residuals.mean():.6f}\")\n",
    "print(f\"Std Dev: {residuals.std():.6f}\")\n",
    "print(f\"Min: {residuals.min():.6f}\")\n",
    "print(f\"Max: {residuals.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Get feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=True)\n",
    "            \n",
    "            # Plot\n",
    "            axes[idx].barh(feature_importance_df['feature'], \n",
    "                          feature_importance_df['importance'],\n",
    "                          color='steelblue')\n",
    "            axes[idx].set_xlabel('Feature Importance', fontsize=11)\n",
    "            axes[idx].set_title(f'{model_name}\\nFeature Importance', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 10 features for best model (if tree-based)\n",
    "if best_model_name in tree_models:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features ({best_model_name}):\")\n",
    "    print(\"=\"*60)\n",
    "    print(feature_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "best_model = trained_models[best_model_name]\n",
    "model_filename = f'tire_degradation_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "model_path = models_dir / model_filename\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"âœ… Best model saved: {model_path}\")\n",
    "\n",
    "# Save all models\n",
    "print(\"\\nSaving all trained models...\")\n",
    "for name, model in trained_models.items():\n",
    "    filename = f'tire_degradation_model_{name.lower().replace(\" \", \"_\")}.pkl'\n",
    "    filepath = models_dir / filename\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"  âœ… {name} -> {filepath.name}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_test_r2': results[best_model_name]['test_r2'],\n",
    "    'best_test_mae': results[best_model_name]['test_mae'],\n",
    "    'best_test_rmse': results[best_model_name]['test_rmse'],\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': X_train.shape[0],\n",
    "    'test_samples': X_test.shape[0],\n",
    "    'features': list(X.columns),\n",
    "    'all_models': {name: {\n",
    "        'test_r2': results[name]['test_r2'],\n",
    "        'test_mae': results[name]['test_mae'],\n",
    "        'test_rmse': results[name]['test_rmse'],\n",
    "        'cv_mean': results[name]['cv_mean'],\n",
    "        'cv_std': results[name]['cv_std']\n",
    "    } for name in results.keys()}\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = models_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Model metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Loading Example (For Future Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load trained model\n",
    "loaded_model = joblib.load(model_path)\n",
    "print(f\"âœ… Model loaded from: {model_path}\")\n",
    "\n",
    "# Test prediction on first 5 test samples\n",
    "sample_predictions = loaded_model.predict(X_test.head())\n",
    "print(f\"\\nSample predictions (first 5 test samples):\")\n",
    "for i, (actual, pred) in enumerate(zip(y_test.head(), sample_predictions)):\n",
    "    print(f\"  Sample {i+1}: Actual={actual:.4f}, Predicted={pred:.4f}, Error={abs(actual-pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test RÂ²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   Test MAE: {results[best_model_name]['test_mae']:.4f} seconds/lap\")\n",
    "print(f\"   Test RMSE: {results[best_model_name]['test_rmse']:.4f} seconds/lap\")\n",
    "print(f\"   CV RÂ² (5-fold): {results[best_model_name]['cv_mean']:.4f} Â± {results[best_model_name]['cv_std']:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Models Trained: {len(trained_models)}\")\n",
    "print(f\"ğŸ“ Models Saved: {len(list(models_dir.glob('*.pkl')))}\")\n",
    "print(f\"ğŸ“ˆ Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"ğŸ“‰ Test Samples: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. ğŸ–¥ï¸  Transfer to Windows for GPU training:\n",
    "   - Follow database transfer instructions in docs/DATABASE_TRANSFER.md\n",
    "   - Install GPU requirements: pip install -r requirements-gpu.txt\n",
    "   - Update XGBoost tree_method to 'gpu_hist' for CUDA acceleration\n",
    "   \n",
    "2. ğŸ”§ Hyperparameter Tuning:\n",
    "   - Use GridSearchCV/RandomizedSearchCV on best models\n",
    "   - Focus on: n_estimators, max_depth, learning_rate, min_samples_split\n",
    "   \n",
    "3. ğŸ”¬ Feature Engineering:\n",
    "   - Create interaction features (e.g., brake_pressure * lateral_g)\n",
    "   - Add polynomial features\n",
    "   - Test lap_in_stint transformations (log, sqrt)\n",
    "   \n",
    "4. ğŸ¯ Model Ensembling:\n",
    "   - Stack top 3 models (VotingRegressor)\n",
    "   - Weighted averaging based on CV scores\n",
    "   \n",
    "5. ğŸš€ Production Script:\n",
    "   - Convert this notebook to src/train_model.py\n",
    "   - Add command-line arguments\n",
    "   - Create src/predict.py for inference\n",
    "   \n",
    "6. ğŸ“ Documentation:\n",
    "   - Document model performance in README.md\n",
    "   - Create model card with metrics and limitations\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
